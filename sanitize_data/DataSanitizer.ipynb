{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sanitizing\n",
    "\n",
    "Used for the data obtained from the data scraper in scrape_twitter.sh with the courtesy of GetOldTweets3 repository\n",
    "\n",
    "Take the old file, make a new file. 1 line should contain: date, time, text, hashtags, @annotations. The text has to be made into lower capital, without links or emoticons, and all words must be utf recognized\n",
    "  \n",
    "Input: tweets from the data scrapper\n",
    "  \n",
    "Output: Date, Time, Text, Hashtags, @annotations in CSV format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Tweet From the Data Scrapper:\n",
    "1610370982123798531 2023-01-03 21:22:16 +0100 <AustinScottGA08> McCarthy earned the right to be the speaker, and any vote against him damages Republicans and our ability to govern.   Those opposing him are putting their selfish motivations over what’s best for our nation.  https://t.co/x2MdTdQlWg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#courtesy of gruber/Liberal Regex Pattern for Web URLs from Github\n",
    "regexWebURLPattern = re.compile(\"\"\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(regexWebURLPattern.match('https://www.google.com'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['McCarthy', 'earned', 'https://google.com', 'for', 'a', 'life', 'time']\n",
      "text McCarthy earned https://google.com for a life time \n",
      "mentions []\n",
      "hastags []\n",
      "date 2023-01-03\n",
      "time 21:22:16\n",
      "filteredTweet: [('2023-01-03', '21:22:16', 'McCarthy earned https://google.com for a life time ', [], [])]\n"
     ]
    }
   ],
   "source": [
    "# tweet = \"1610370982123798531 2023-01-03 21:22:16 +0100 <AustinScottGA08> McCarthy earned the right to be the speaker, and any vote against him damages Republicans and our ability to govern.   Those opposing him are putting their selfish motivations over what’s best for our nation.  https://t.co/x2MdTdQlWg\"\n",
    "\n",
    "tweet = \"1610370982123798531 2023-01-03 21:22:16 +0100 <AustinScottGA08> McCarthy earned https://google.com for a life time\"\n",
    "def tweetDecomposer(tweet):\n",
    "    #seperate by word\n",
    "    tweetWords = tweet.split()\n",
    "    # print (tweetWords)\n",
    "    #remove tweet ID\n",
    "    tweetWords = tweetWords[1:]\n",
    "    # print(tweetWords)\n",
    "\n",
    "    date = tweetWords[0]\n",
    "    time = tweetWords[1]\n",
    "    timezone = tweetWords[2]\n",
    "\n",
    "    #remove time/timezone and tweet username\n",
    "    tweetWords = tweetWords[4:]\n",
    "\n",
    "    mentions = []\n",
    "    hashtags = []\n",
    "    text = \"\"\n",
    "\n",
    "    print(tweetWords)\n",
    "    for tweetWord in tweetWords:\n",
    "        if regexWebURLPattern.search(tweetWord):\n",
    "            print('matched')\n",
    "            break #ignore Web URL\n",
    "        elif tweetWord[0] == \"@\":\n",
    "            mentions.append(tweetWord[1:])\n",
    "        elif tweetWord[0] == \"#\":\n",
    "            hashtags.append(tweetWord[1:])\n",
    "        else:\n",
    "            text += tweetWord + \" \"\n",
    "    return date, time, text, mentions, hashtags\n",
    "\n",
    "filteredTweets = []\n",
    "\n",
    "filteredTweets.append(tweetDecomposer(tweet))\n",
    "print(\"filteredTweet:\", filteredTweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV File from the sanitized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('filteredTweets.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Date\", \"Time\", \"Text\", \"Mentions\", \"Hashtags\"])\n",
    "    for filteredTweet in filteredTweets:\n",
    "        writer.writerow([filteredTweet[0], filteredTweet[1], filteredTweet[2], filteredTweet[3], filteredTweet[4]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
