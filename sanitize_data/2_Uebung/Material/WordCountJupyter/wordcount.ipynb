{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop WordCount Example in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight adaptions to the code from last week to enable local processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new `mapper.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys \n",
    "\n",
    "def mapper(textinput):\n",
    "    # input comes from STDIN (standard input)\n",
    "    returnstring = \"\"\n",
    "    for line in textinput.splitlines():\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line into words\n",
    "        words = line.split()\n",
    "        # increase counters\n",
    "        for word in words:\n",
    "            # write the results to STDOUT (standard output);\n",
    "            # what we output here will be the input for the\n",
    "            # Reduce step, i.e. the input for reducer.py\n",
    "            #\n",
    "            # tab-delimited; the trivial word count is 1\n",
    "            returnstring = returnstring + ('%s\\t%s\\n' % (word, 1))\n",
    "    print (returnstring)\n",
    "    return returnstring\n",
    "\n",
    "mapper(sys.stdin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new `reducer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\t0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'None\\t0\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys \n",
    "from operator import itemgetter\n",
    "\n",
    "         \n",
    "def reducer(textinput):\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "    returnstring = \"\"\n",
    "    # input comes from STDIN\n",
    "    for line in textinput.splitlines():\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        \n",
    "        #print(\"»\" + line + \"»\")\n",
    "        \n",
    "        # parse the input we got from mapper.py\n",
    "        try:\n",
    "            word, count = line.split('\\t', 1)\n",
    "        except ValueError:\n",
    "            # e.g. empty line\n",
    "            continue\n",
    "\n",
    "        # convert count (currently a string) to int\n",
    "        try:\n",
    "            count = int(count)\n",
    "        except ValueError:\n",
    "            # count was not a number, so silently\n",
    "            # ignore/discard this line\n",
    "            continue\n",
    "\n",
    "        # this IF-switch only works because Hadoop sorts map output\n",
    "        # by key (here: word) before it is passed to the reducer\n",
    "        if current_word == word:\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_word:\n",
    "                # write result to STDOUT\n",
    "                returnstring = returnstring + ('%s\\t%s\\n' % (current_word, current_count))\n",
    "            current_count = count\n",
    "            current_word = word\n",
    "\n",
    "    # do not forget to output the last word if needed!\n",
    "    if current_word == word:\n",
    "        returnstring = returnstring + ('%s\\t%s\\n' % (current_word, current_count))\n",
    "    print (returnstring)\n",
    "    return returnstring\n",
    "            \n",
    "\n",
    "\n",
    "reducer(sys.stdin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo\t1\n",
      "hallo\t1\n",
      "hallo\t1\n",
      "ich\t1\n",
      "bin\t1\n",
      "ein\t1\n",
      "Test\t1\n",
      "Hallo\t1\n",
      "ich\t1\n",
      "bin\t1\n",
      "noch\t1\n",
      "ein\t1\n",
      "Test\t1\n",
      "\n",
      "Hallo\t1\n",
      "hallo\t2\n",
      "ich\t1\n",
      "bin\t1\n",
      "ein\t1\n",
      "Test\t1\n",
      "Hallo\t1\n",
      "ich\t1\n",
      "bin\t1\n",
      "noch\t1\n",
      "ein\t1\n",
      "Test\t1\n",
      "\n",
      "Hallo\t1\n",
      "hallo\t2\n",
      "ich\t1\n",
      "bin\t1\n",
      "ein\t1\n",
      "Test\t1\n",
      "Hallo\t1\n",
      "ich\t1\n",
      "bin\t1\n",
      "noch\t1\n",
      "ein\t1\n",
      "Test\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = mapper(\"Hallo hallo hallo ich bin ein Test\\nHallo ich bin noch ein Test\")\n",
    "output = reducer(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo\t1\n",
      "hallo\t1\n",
      "hallo\t1\n",
      "ich\t1\n",
      "bin\t1\n",
      "ein\t1\n",
      "Test\t1\n",
      "Hallo\t1\n",
      "ich\t1\n",
      "bin\t1\n",
      "noch\t1\n",
      "ein\t1\n",
      "Test\t1\n",
      "\n",
      "Hallo\t2\n",
      "Test\t2\n",
      "bin\t2\n",
      "ein\t2\n",
      "hallo\t2\n",
      "ich\t2\n",
      "noch\t1\n",
      "\n",
      "Hallo\t2\n",
      "Test\t2\n",
      "bin\t2\n",
      "ein\t2\n",
      "hallo\t2\n",
      "ich\t2\n",
      "noch\t1\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "output = mapper(\"Hallo hallo hallo ich bin ein Test\\nHallo ich bin noch ein Test\")\n",
    "output=sorted(output.split('\\n'))\n",
    "output='\\n'.join(output)\n",
    "output = reducer(output)\n",
    "print(output)\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionalität im Cluster testen: \n",
    "\n",
    "\n",
    "`time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -combiner reducer.py -input test_dir -output result_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
